{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red222\green226\blue234;\red222\green226\blue234;
\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c89412\c90980\c93333;\cssrgb\c89412\c90980\c93333;
\cssrgb\c100000\c100000\c100000;}
\paperw11900\paperh16840\margl1440\margr1440\vieww13040\viewh9020\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 REGRESSION PART 1\
\
Question a: \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Describe the main properties of the data, focusing on the size, data ranges,\cf0 \strokec0 \
\pard\pardeftab720\sl200\partightenfactor0
\cf2 \strokec2 and data types.\cf0 \strokec0 \
\
\
\
The attributes are revision_time (a 64-bit floating-point number) and the target is exam_score (a 64-bit floating point number). Across the dataset, exam_score ranges from a minimum of 14.731 to a maximum of 94.945 while revision_time \cb4 \outl0\strokewidth0 ranges from a minimum value of 2.723 to a maximum of 48.011.\
\
\
Question b: w = [17.897680, 1.441141]. phi[0] represents the score you get if you do not study (revision_time = 0) while phi[1] represents the gradient or slope of the regression line.\
\
Question c: See figure in directory\
\
Question d: Get from jupyter notebook\
\
Question e: Limitation of MSE - the mean squared error is prone (sensitive) to outliers.\
\
Question f: MSE for linear model fitted using sklearn : error = \cf2 \cb5 \outl0\strokewidth0 \strokec2 30.9854726145413
\f1\fs28 \

\f0\fs24 \cf0 \cb4 \outl0\strokewidth0 	        MSE for closed-form solution : error = 30.985472614541287\
\
Question g: The resulting graph is a parabola whose minimum is 32.48 \cb4 (to 2 d.p.)\cb4  along the ordinate, corresponding to a value of 1.35 (to 2 d.p.) along the abscissa. Yes this is the expected value given that for the actual optimal value of w0 (17.9 to 1 d.p.), the optimal value of w1 is 1.44 (to 2 d.p.) which is in the neighbourhood of 1.35.\
\
NONLINEAR REGRESSION\
\
Question a: See figure in directory\
\
Question b: See figure in directory\
\
Question c: \cb4 The MSE of M=4 is smaller than that of M=3. M=4 suffers from overfitting as it is sensitive to noise in the data and so M=4 performs better than M=3. I would go for M=3 because it will yield better results on unseen data than M=4.\cb4 \
\
RBFs\
\
Question d: Low values of alpha lead to severe underfitting (alpha=0.2 has mse of 108.358) while larger ones lead to overfitting (a=1000 has mse of 18.818). A value of alpha=25 captures the patterns in the data very well (has mse of 2.745).\cf2 \cb5 \outl0\strokewidth0 \strokec2 \
\
DECISION TREES\
\
Question a:\
\
The training dataset has 4800 training inputs each with 136 feature attributes. The target class is a binary (takes values of either 0 or 1) and has datatype 64-bit integer. All of the training input attributes are 64-bit floating point numbers. \
\
\pard\pardeftab720\sl200\partightenfactor0
\cf2 \outl0\strokewidth0 The testing dataset has 1200 training inputs each with 136 feature attributes. The target class is a binary (takes values of either 0 or 1) and has datatype 64-bit integer. All of the testing input attributes are 64-bit floating point numbers. 
\f1\fs28 \outl0\strokewidth0 \
\pard\pardeftab720\sl200\partightenfactor0

\f0\fs24 \cf2 \
\cf0 \cb4 \outl0\strokewidth0  Question b:\
\
The data points for smiling faces are further away from each other (more \cb4 spread out) around the lips and chin as compared to those for the not smiling faces. \
\
Question c:\
\
Sklearn uses gini to measure the purity of a node. The gini index is faster to compute as opposed to entropy because entropy makes use of the logarithm for its computation.\
\
Question d: Low values of max_depth lead to underfitting. High values of max_depth will cause the model to overfit to the training data and thus will not be able to generalise on the testing data and also leads to complex data models.\
\
Question e: \
\
Tree of max-depth of 2 : Training accuracy is 79.48% and Testing accuracy is 78.17%\
Tree of max-depth of 8: Training accuracy is 93.35% and Testing accuracy is 84.08%\
Tree of max-depth of 20: Training accuracy is 100% and Testing accuracy is 81.58%\
\
Model of tree with max-depth of 8 is the best because it performs best on the testing data - that is it is better at generalising as compared to the the other 2 models.\
\
Question f:\
\
Most important attributes in descending order of Gini importance:\
\
\'91x50\'92 -> 0.33040, \'91x48\'92 -> 0.08996, \'91y29\'92 -> 0.08831 (all to 5 d.p)\
\
The mean value (for the training data) of the 2D coordinate for the point containing the most important attribute on a smiling face is (-0.22, 0.04) and that for a non smiling face is (-0.18, 0.04). From the scatter plot in Question (b), we see that this corresponds to a point on the upper lip of a face. It makes sense that this is the most important feature as we notice that the lips dilate when a person smiles.\
\
Question g: Yes there are. A point on the human face is a 3D coordinate in the real world but it is represented as a 2D coordinate in our dataset. This representation leads to loss of information as we have ignored the third dimension which could potentially play a huge part (as in providing us with features of higher importance) in the classification task.\
\
Question 4\
\
a)Classification accuracy for algorithms:\
\
alg_1 : 61.6%, alg_2 : 55.0%, alg_3 : 32.1%, alg_4 : 32.9%\
\
So model 1 is the best according to this metric.\
\
One limitation is that this measure is that it does not work well with a class-imbalanced dataset. It can be improved by using an equal number of data points for each class during training and testing.\
\
b) AUC for models:\
\
Model 1 : 0.6799 (to 4 d.p)\
Model 2 : 0.5997\
Model 3 : 0.2011\
Model 4 : 0.5796\
\
Yes the model with the highest ROC score is also the one with the highest accuracy.\
\
c) Though setting a very low threshold (one very close to 0) will increase the true positive rate of alg_3, its false positive rate would sell be very high (close to 1.0). Even when the threshold is 0.01, the accuracy of alg_3 is 36% which indicates that the model 3 has to be retrained.\
}